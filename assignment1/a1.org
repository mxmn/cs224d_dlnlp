* Definitions
- row vectors:
- 1 layer network:
  - Input dimension: D_x  ($\x\in[N, D_x]$)
  - Hidden units: $H$  ($\W_1\in [D_x, H],\ \b_1\in [H]$)
  - Ouput dimension: D_y  ($\W_2\in [H, D_y],\ b_2\in [D_y]$)

* Assigment 1

Org latex export is not that great yet, and one still has to work on
the alignment.


** 1.a) Softmax function $\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j}e^{x_j}}$

  \[\text{softmax}(x+c)_i = \frac{e^{x_i + c}}{\sum_{j}e^{x_j+c}} =
  \frac{e^{c}e^{x_i}}{e^{c}\sum_{j}e^{x_j}} =
  \frac{e^{x_i}}{\sum_{j}e^{x_j}} = \text{softmax}(x)_i\]

** 2.a) Derivative of the sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}}$

  $$\pp{}{x}\sigma(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{1}{1+e^{-x}}\, \frac{e^{-x}}{1+e^{-x}} = \frac{1}{1+e^{-x}}\, \frac{(1 + e^{-x}) -1}{1+e^{-x}} = \sigma(x)(1-\sigma(x))$$

** 2.b) Derivative of the Cross Entropy Loss $CE(\y,\hat\y) = -\sum_{i}y_{i} \log(\hat y_{i})$

  $$\hat\y = \text{softmax}(\theta)$$

  $$\pp{}{\theta_j} CE(\y,\hat\y)_j = - \pp{}{\theta_j} \sum_i y_i\log
  \hat y_i = - \pp{}{\theta_j} \sum_i y_i \log(e^{\theta_i}/\sum_l
  e^{\theta_l})
  =-\pp{}{\theta_j} \sum_i y_i (\theta_i - \log\sum_l e^{\theta_l})$$

  $\y$ is a $one-hot$ label vector with non-zero value at index $k$.

  If $j\not=k$: $$\pp{}{\theta_j} CE(\y,\hat\y)_j = \pp{}{\theta_j}
  \log\sum_l e^{\theta_l} = \hat y _j$$

  If $j=k$:  $$\pp{}{\theta_j} CE(\y,\hat\y)_j =  \hat y_j - 1$$

  given that $$\pp{}{\theta_j} \log\sum_l e^{\theta_l} =
  \frac{e^\theta_j}{\sum_l e^{\theta_l}} = \hat y_j$$

** 2.c) Gradient of the one-layer network

   $$\pp{J(x)}{x} = \pp{}{x} CE(\y,\hat\y) = -\pp{}{x} \sum_i y_i\log
   (\text{softmax}(\sigma(\x\W_1+\b_1)\W_2+\b_2)) $$

   With $\hat \y=\text{softmax}(\theta)$,
   $\theta=\sigma(\h)\W_2+\b_2$ and $\h=\x\W_1+\b_1$

   $$\pp{J(x)}{x} = \sum_j \pp{J}{\theta_j} \pp{\theta_j}{x} =
   \sum_j \sigma(\h)(1-\sigma(\h))\W_1\W_{2,j}
   \times \begin{cases}
   \hat y_j& \text{if }j\not=k\\
   \hat y_j-1 & \text{if }j=k \end{cases}$$

** 2.d) Number of parameters
   Number of parameters: $(D_x+1) H + (H+1) D_y$


* COMMENT
#+TITLE: Assignment 1 Notes
#+DATE:
